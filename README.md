ğŸš€ MMSD LLM Benchmarking

A benchmarking framework for evaluating Multimodal Large Language Models (MLLMs) on MMSD-based  tasks.

This repository focuses on systematically evaluating how well modern MLLMs perform on multimodal OSINT (Open-Source Intelligence) scenarios using the MMSD dataset, covering reasoning, perception, grounding, and cross-modal understanding.

ğŸ“Œ Project Goals

Benchmark MLLM performance on real-world OSINT-style tasks

Evaluate vision + language reasoning together

Provide reproducible metrics and evaluation pipelines

Compare multiple LLM / MLLM backends fairly

ğŸ§  Key Features

ğŸ“Š Standardized benchmarking pipeline

ğŸ–¼ï¸ Multimodal (text + image) evaluation

ğŸ¤– Support for multiple LLM / MLLM providers

ğŸ“ˆ Metric-based comparison (accuracy, faithfulness, reasoning quality, etc
